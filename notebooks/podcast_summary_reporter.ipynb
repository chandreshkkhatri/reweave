{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912aa1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt-dlp in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (2025.6.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73afd82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set AssemblyAI API Key\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# ❷ Read the key-value pairs and add them to os.environ\n",
    "load_dotenv()      # ← now the vars are set\n",
    "\n",
    "# print(f\"AssemblyAI API key is {os.environ['ASSEMBLYAI_API_KEY']} !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# ─── AssemblyAI Configuration ─────────────────────────────────────────────────\n",
    "\n",
    "API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Please set your ASSEMBLYAI_API_KEY environment variable\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": API_KEY,  # Changed from \"authorization\" to \"Authorization\"\n",
    "    \"Content-Type\": \"application/json\"  # Changed from \"content-type\" to \"Content-Type\"\n",
    "}\n",
    "TRANSCRIBE_URL = \"https://api.assemblyai.com/v2/transcript\"\n",
    "\n",
    "# ─── Transcription Function ────────────────────────────────────────────────────\n",
    "\n",
    "def transcribe_url(\n",
    "    audio_url: str,\n",
    "    speakers_expected: int = None,\n",
    "    poll_interval: int = 5,\n",
    "    timeout: int = 600\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Submit an audio file URL to AssemblyAI for transcription and poll until completion.\n",
    "    \n",
    "    Args:\n",
    "        audio_url: Direct URL to the audio file (MP3, WAV, etc.)\n",
    "        speakers_expected: The number of speakers to detect in the audio.\n",
    "        poll_interval: Seconds between polling requests (default: 5)\n",
    "        timeout: Maximum seconds to wait before giving up (default: 600)\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcribed text with speaker labels.\n",
    "        \n",
    "    Raises:\n",
    "        requests.HTTPError: For API request issues\n",
    "        RuntimeError: For transcription errors\n",
    "        TimeoutError: If transcription takes too long\n",
    "    \"\"\"\n",
    "    print(f\"Starting transcription for: {audio_url}\")\n",
    "    \n",
    "    # Set up transcription parameters\n",
    "    json_payload = {\"audio_url\": audio_url}\n",
    "    if speakers_expected:\n",
    "        json_payload[\"speaker_labels\"] = True\n",
    "        json_payload[\"speakers_expected\"] = speakers_expected\n",
    "\n",
    "    # Submit transcription request\n",
    "    response = requests.post(\n",
    "        TRANSCRIBE_URL,\n",
    "        json=json_payload,\n",
    "        headers=HEADERS\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    transcript_id = response.json()[\"id\"]\n",
    "    print(f\"Transcription job started with ID: {transcript_id}\")\n",
    "\n",
    "    # Poll for completion\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        poll_resp = requests.get(f\"{TRANSCRIBE_URL}/{transcript_id}\", headers=HEADERS)\n",
    "        poll_resp.raise_for_status()\n",
    "        data = poll_resp.json()\n",
    "        status = data.get(\"status\")\n",
    "\n",
    "        print(f\"Status: {status}\")\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            print(\"Transcription completed!\")\n",
    "            if data.get('utterances'):\n",
    "                # Format the transcript with speaker labels\n",
    "                transcript = \"\"\n",
    "                for utterance in data['utterances']:\n",
    "                    speaker = utterance['speaker']\n",
    "                    text = utterance['text']\n",
    "                    transcript += f\"Speaker {speaker}: {text}\\n\"\n",
    "                return transcript\n",
    "            else:\n",
    "                return data.get(\"text\", \"\")\n",
    "        elif status == \"error\":\n",
    "            error_msg = data.get(\"error\", \"Unknown error\")\n",
    "            raise RuntimeError(f\"Transcription failed: {error_msg}\")\n",
    "\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"Transcription timed out after {timeout} seconds\")\n",
    "\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# ─── Usage Example ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# Example usage:\n",
    "# audio_url = \"https://example.com/podcast.mp3\"\n",
    "# transcript = transcribe_url(audio_url, speakers_expected=2)\n",
    "# print(\"Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae5d484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── YouTube Transcription Setup ──────────────────────────────────────────────\n",
    "\n",
    "# Replace with your actual YouTube URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=WjKQQAFwrR4\"\n",
    "\n",
    "# Required imports for YouTube transcription\n",
    "import yt_dlp\n",
    "import tempfile\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def get_youtube_video_info(youtube_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts metadata from a YouTube video.\n",
    "    \n",
    "    Args:\n",
    "        youtube_url: The URL of the YouTube video.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing video metadata.\n",
    "    \"\"\"\n",
    "    ydl_opts = {'quiet': True}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(youtube_url, download=False)\n",
    "        return {\n",
    "            'title': info_dict.get('title'),\n",
    "            'description': info_dict.get('description'),\n",
    "            'uploader': info_dict.get('uploader'),\n",
    "            'tags': info_dict.get('tags'),\n",
    "        }\n",
    "\n",
    "def transcribe_youtube_video(\n",
    "    youtube_url: str,\n",
    "    poll_interval: int = 5,\n",
    "    timeout: int = 600\n",
    ") -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Download and transcribe YouTube audio using AssemblyAI.\n",
    "    \n",
    "    Args:\n",
    "        youtube_url: YouTube video URL\n",
    "        poll_interval: seconds between polling\n",
    "        timeout: max seconds to wait\n",
    "    Returns:\n",
    "        A tuple containing the transcript text and a dictionary of video metadata.\n",
    "    \"\"\"\n",
    "    print(f\"Processing YouTube video: {youtube_url}\")\n",
    "    \n",
    "    # Extract video metadata\n",
    "    video_info = get_youtube_video_info(youtube_url)\n",
    "    print(f\"Video Title: {video_info.get('title')}\")\n",
    "    \n",
    "    # Create a temporary directory to store the download\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        download_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': os.path.join(tmpdir, 'audio'), # Use a fixed name inside the temp dir\n",
    "            'noplaylist': True,\n",
    "        }\n",
    "        \n",
    "        audio_file = None\n",
    "        with yt_dlp.YoutubeDL(download_opts) as ydl:\n",
    "            # Download the file\n",
    "            ydl.download([youtube_url])\n",
    "            \n",
    "            # Find the downloaded file (yt-dlp adds the extension)\n",
    "            for entry in os.listdir(tmpdir):\n",
    "                if entry.startswith('audio'):\n",
    "                    audio_file = os.path.join(tmpdir, entry)\n",
    "                    break\n",
    "        \n",
    "        if not audio_file or not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:\n",
    "            raise Exception(f\"Failed to download audio file from YouTube.\")\n",
    "    \n",
    "        print(f\"Downloaded to {audio_file} ({os.path.getsize(audio_file)} bytes)\")\n",
    "        \n",
    "        # Upload to AssemblyAI\n",
    "        print(\"Uploading audio to AssemblyAI...\")\n",
    "        try:\n",
    "            upload_url = upload_file_to_assemblyai(audio_file)\n",
    "            print(f\"Uploaded URL: {upload_url}\")\n",
    "            \n",
    "            # Get transcript\n",
    "            transcript = transcribe_url(upload_url, poll_interval=poll_interval, timeout=timeout)\n",
    "            return transcript, video_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "def upload_file_to_assemblyai(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Uploads a local audio file to AssemblyAI and returns a public URL for transcription.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        raise ValueError(f\"File is empty or does not exist: {file_path}\")\n",
    "    \n",
    "    print(f\"Uploading file: {file_path} (size: {os.path.getsize(file_path)} bytes)\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        resp = requests.post(\n",
    "            'https://api.assemblyai.com/v2/upload',\n",
    "            headers={'Authorization': API_KEY},\n",
    "            data=f\n",
    "        )\n",
    "    \n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get('upload_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87f8815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting YouTube video transcription...\n",
      "Processing YouTube video: https://www.youtube.com/watch?v=WjKQQAFwrR4\n",
      "Video Title: How Much Memory for 1,000,000 Threads in 7 Languages | Go, Rust, C#, Elixir, Java, Node, Python\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=WjKQQAFwrR4\n",
      "[youtube] WjKQQAFwrR4: Downloading webpage\n",
      "Video Title: How Much Memory for 1,000,000 Threads in 7 Languages | Go, Rust, C#, Elixir, Java, Node, Python\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=WjKQQAFwrR4\n",
      "[youtube] WjKQQAFwrR4: Downloading webpage\n",
      "[youtube] WjKQQAFwrR4: Downloading tv client config\n",
      "[youtube] WjKQQAFwrR4: Downloading tv client config\n",
      "[youtube] WjKQQAFwrR4: Downloading tv player API JSON\n",
      "[youtube] WjKQQAFwrR4: Downloading tv player API JSON\n",
      "[youtube] WjKQQAFwrR4: Downloading ios player API JSON\n",
      "[youtube] WjKQQAFwrR4: Downloading ios player API JSON\n",
      "[youtube] WjKQQAFwrR4: Downloading m3u8 information\n",
      "[youtube] WjKQQAFwrR4: Downloading m3u8 information\n",
      "[info] WjKQQAFwrR4: Downloading 1 format(s): 251\n",
      "[info] WjKQQAFwrR4: Downloading 1 format(s): 251\n",
      "[download] Destination: /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio\n",
      "[download]   0.0% of   24.18MiB at  274.23KiB/s ETA 01:30[download] Destination: /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio\n",
      "[download] 100% of   24.18MiB in 00:00:01 at 14.60MiB/s    \n",
      "\n",
      "Downloaded to /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio (25352038 bytes)\n",
      "Uploading audio to AssemblyAI...\n",
      "Uploading file: /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio (size: 25352038 bytes)\n",
      "Downloaded to /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio (25352038 bytes)\n",
      "Uploading audio to AssemblyAI...\n",
      "Uploading file: /var/folders/1p/vtrslkcd30vfb7ndv5b9c5s40000gp/T/tmp0y2s9hq3/audio (size: 25352038 bytes)\n",
      "Uploaded URL: https://cdn.assemblyai.com/upload/6074f4d9-f410-4400-b607-88cd6d9b76db\n",
      "Starting transcription for: https://cdn.assemblyai.com/upload/6074f4d9-f410-4400-b607-88cd6d9b76db\n",
      "Uploaded URL: https://cdn.assemblyai.com/upload/6074f4d9-f410-4400-b607-88cd6d9b76db\n",
      "Starting transcription for: https://cdn.assemblyai.com/upload/6074f4d9-f410-4400-b607-88cd6d9b76db\n",
      "Transcription job started with ID: 261a3577-0aeb-4660-b9e8-21205c1ab5e5\n",
      "Transcription job started with ID: 261a3577-0aeb-4660-b9e8-21205c1ab5e5\n",
      "Status: processing\n",
      "Status: processing\n",
      "Status: processing\n",
      "Status: processing\n",
      "Status: processing\n",
      "Status: processing\n",
      "Status: completed\n",
      "Transcription completed!\n",
      "\n",
      "==================================================\n",
      "VIDEO METADATA:\n",
      "==================================================\n",
      "Title: How Much Memory for 1,000,000 Threads in 7 Languages | Go, Rust, C#, Elixir, Java, Node, Python\n",
      "Description: Recorded live on twitch, GET IN \n",
      "\n",
      "https://twitch.tv/ThePrimeagen\n",
      "\n",
      "ty piotr!\n",
      "https://pkolaczk.github.io/memory-consumption-of-async/\n",
      "\n",
      "MY MAIN YT CHANNEL: Has well edited engineering videos\n",
      "https://youtube.com/ThePrimeagen\n",
      "\n",
      "Discord\n",
      "https://discord.gg/ThePrimeagen\n",
      "\n",
      "\n",
      "Have something for me to read or react to?: https://www.reddit.com/r/ThePrimeagenReact/\n",
      "Uploader: ThePrimeTime\n",
      "Tags: ['programming', 'computer', 'software', 'software engineer', 'software engineering', 'program', 'development', 'developing', 'developer', 'developers', 'web design', 'web developer', 'web development', 'programmer humor', 'humor', 'memes', 'software memes', 'engineer', 'engineering', 'Regex', 'regexs', 'regexes', 'netflix', 'vscode', 'vscode engineer', 'vscode plugins', 'Lenovo', 'customer service']\n",
      "\n",
      "==================================================\n",
      "TRANSCRIPT:\n",
      "==================================================\n",
      "How much memory do you need to run 1 million concurrent tasks? By the way, alerts are off. In this blog post, I delve into the comparison of memory consumption between asynchronous and multi threaded programs across popular languages like Rust, Go, Java, C Sharp, Python, no js, and of course, everybody's favorite. These nuts. Sometimes ago I had to compare performance of a few computer programs designed to handle a large number of network connections. I saw a huge difference in memory consumption of those programs even exceeding 20x. Okay, that. I mean, it makes sense. Well, you know, how are they handling things? Because like in Go, it's pretty. It's pretty slick. Goes pretty dang. Pretty dang slick. You know what I mean? Some programmers consume little over 100 megabytes, but others reach almost three gigs. I think what he's trying to say here, I think what Peter. What old Peter up there, Kalaskowski, is trying to say is that node js, node js loves the memory, okay? At 10k connections. Unfortunately, those programs were quite complex and differed also in features. So it'd be hard to compare them directly and draw some meaningful conclusion as that wouldn't be an Apple to Apple comparison. This led me to an idea of creating a synthetic benchmark instead. Synthetic. By the way, I actually do want to like follow this, this little line of thought right here. So I want everyone to pay really close attention. If you go to GitHub.com the Primogen, right? And you look at this and you go to my latest one. Is there like. Yeah, not left pad. There we go. T.S. rust. Zig. D's. Yes. I'm. I'm going through a D's phase. Okay. If you go here, you can join and add your own language. We're building the interpreter, okay. And then I want to test it. I actually want to build a server and that does like full interpretation and we'll figure out what the, what the rules are and all that, but actually can compile remotely, right? And then that is what language can do that the fastest. I feel like that is a way cooler, like, test of languages than these dumb tests that you see, right? There's that one YouTube video that did like, that just did like a mathematical formula and be like, which one does this one the best? And it's just like. But that's not even like real at all. Okay, you're not even creating mem. We need like, like memory. You need cleanup, you need things that happen. You need connections, you need syscalls. You want to see the entire, you know, the entire Thing you don't want to just see. Like just some tiny little nothing. You know what I mean? It never works that way. It's always a lie. It's just a lie. It's just always a lie. Anyways, let's do this. Yeah. Microsoft guy. Yeah. Benchmark. I created the following program in various programming languages. Let's launch n concurrent tasks where each tackle waits for 10 seconds and then the program exists after all the tasks finish. What? The program continues to exist after you're finished. I'm pretty sure this is like a fork bomb. Then the number of tasks is controlled by the command line argument. Let's go. Let's go. The Little help of Chat Jeopardy. I could write such programs in a few. What? What? What do you mean this is not hard. And Rust. I mean are you using a real threat? Are using hardware threads or green threads or Go? That's simple. Java. This is probably not that hard. Could probably do it in a few moments of just a. What do you mean chat? Shippity. See this seems like such a crazy crazy things. People just love them. Chad Jippities Anyways, Rust. I created three programs in Rust. The first one uses traditional threads. That's what I wanted to see. Okay. I wanted to see one hardware in one language. Traditional threads. Okay. I'm six of all this polythreadisms going on. Here's the core of it. Bam. Make it happen. The other two versions had Async with Tokyo and the other with Async stood. Here's the core of the Tokyo. Beautiful. Beautiful. I like this. The Async stud variant is very similar, so I won't put it there. Go Go routines are the building block for conservancy so we don't need to do them separately. But we used a weight group. Okay. Beautiful. Great use of a weight group by the way. This is a great use of a weight Awake group. Loved it. Loved every part of this. Again, I think Go for all of its downfalls also has a lot of updrafts. What's. What's the term for like what's the opposite of a downfall? Yes, I did have a bad experience with C Sharp. Why you bring that up? Okay. An upfall, an upwind, an updraft, an uplift, a boob job. What do we call it? I don't even know Java. Java traditionally uses threads, but JD JDK21 offers a preview of virtual threads which are similar concept to Go routines. Look at Java Go. Java has almost caught up to 2015. I think we all need to Be impressed right now. Can we just take a moment to realize that Java's going places? It really is like this is incredible. I mean the next thing you know they, I mean the next. They could actually beat C. By the time C23 comes out, Java might actually be exceeding what happened in 2023. As far as techno technologies go, it could be, it could be incredible. And I think you guys are just not even considering how amazing this is. All right, let's see. List of threads, new array list. Good choice of arraylist. You did not create, you know, a little alarming that you didn't, you know, pre populate the size here. You knew how big it was going to be. A bunch of things. New threads. Alright. Little sleep, little dry catch does it. So he's not counting. One thing I'd be a little careful of is even though you don't need to, there is no counting right here going on. Right. How many times did this fail? I think that's really important to think about. You know, just in case when you're creating these tests you, you need to do that. Okay, thread start, thread add. Let's go. Thread join. Perfect. Beautiful. I'm curious about this as well. When you go individual thread joins, this could be bad, right? Am I, am I right on this one? This could actually have a, a greater slowdown because you're, you're doing it one at a time and so you could have already had like a hundred finished that you like aren't. You know what I mean? That's a race. Yeah, yeah. I, I feel like this is, it's blocking. I know that, but it's, it's not just blocking, it's that you could pay a huge penalty up front. Then you'll have this whole thing where you have to go through each one and I assume also join is like not a free call. I assume it's like a syscall. Is that fair? I don't know. I don't know what it takes to do a join. But my, my assumption is that it's a non trivial cost, you know? I mean. Yeah. Anyways, just a thought. Just a thought. But I guess if they're green threads, maybe that's not the case. Maybe they're actually really fast and delicious. You know what I mean? I don't know. Anyways, these are just my thoughts. I'm just kind of raw, dogging my thoughts out here. Okay, here's the, the variant with virtual threads. Notice how similar they are. Oh, nice. That's beautiful. Well done. Oh, I guess these Ones are hardware ones and the other one. These ones are hardware ones. These ones are. Oh, my goodness, I'm scrolling way too fast. Oh, my goodness, I'm gonna vomit. These ones are virtual ones. Beautiful. Still doing that. I don't like that. C Sharp is similar to Rust, has first class support for Async Await. Awesome. So there we go. We're gonna do a little task run right here. So this must be. Okay, so this has to be. Those have to be synthetic threads. So we're not actually getting. So since C Sharp is not doing hardware threads, so there does need to be like a. A disambiguation here. Which one's doing, like actual. Actual threads versus which ones are doing some sort of managed environment. Because I would assume that the managed environment. This is where a managed environment does. Amazing. Right? Right? I think so. It can do both. Well, I would assume whenever you see something like Async Await, usually there's some sort of whole managed environment that goes into it. Usually. Right. Because if you're not doing it yourself, there's something else going on. You know what I mean? I feel like the managed environment would be slower. No, because hardware. Hardware threads are expensive. Right? No. J.S. utils promised. Okay, so again, you don't want to do this. I feel like there is a little something there you got to be careful about. You know, maybe a little extra garbage collection going on. I don't know. But probably fine. Probably not a big deal. Probably enough. Probably not. Oh, my goodness. Probably not enough to be too upset about it. Python. I don't know what delay is. Delay must just be a function that returns a. Oh, set timeout. Oh, yeah. Okay. Yeah, yeah. I'm dumb. I'm dumb. This is also a syntax error, guys. Syntax error right there. Okay. Watch your parents. Okay. Don't. You should make sure you always just put code up on an article. That's just like. It works. Always make sure. I mean, I've done it too. We've all done it. Just make sure it works. Right? Okay, so this is looking good. Python added Async Await. And three, five. Nice. Okay, look at this. Asyncio. Asynchronous. There we go. Beautiful. That all looks great. Elixir is famous for Async capabilities as well. Let's go. I love to see it. Task, await, many, until infinity and beyond. You know, I really hate that phrase by Buzz Lightyear. To infinity and beyond. Wouldn't you not be at infinity if you could go beyond the point of infinity? Isn't that just like. Not infinity like can we be real here? That's the joke. It's true also Infinity. Yeah, I know. Thank you. Deep thoughts with me. Just letting you know. Deep thoughts. All right. Test environment. Hardware. Xeon. This thing. Okay, this is starting to look like a personal computer, which again, gotta be a little careful. Rust 169. Nice go. 18. 1. Nice. JDK dotnet node, Python Elixir. Let's go. All programs were launched using release mode if available. Other options were left default. All right, minimum footprint. Let's start with something small because some of the runtimes require some memory for themselves. Yep, that's right. Yep. Here we go. This makes sense though, because Node would be really, really large. Well, C Sharp requires 131 megabytes for just nothing. What Like I get that there's like a whole thing going on, but that's just a lot. I mean this all seems about in line with Node, right? C Sharp. I want. What does C Sharp do that requires three times the amount of telemetry information that the other ones do? Okay, it's Pindos anyways, so go. Make sense, Rust. All these things make sense, right? Because they all should be really, really small because they're actually. You know, these are actually like compiled thingies. But I would like to say that this is really impressive for Go. I want you to take a. A moment about this. This includes a garbage collector. Okay. Like things are running here. That's really good. That's really, really good. Good job Go. Good job, Go. That's really, really good. The. The surprise here was Python. It shouldn't be surprising. Oh, I mean, I guess it's surprising in the sense that it's half the. It's half the memory of. I guess I would have. I guess in my head I would have probably put it the same as Java or no JS Elixir. Also a little surprising. It's so large. Interesting. I wonder how he's getting the memory. Is he using vm rss? What is he using? Here we can see there's are certainly two groups of programs. Go and Rust. Programs compile statically to native binaries need very little memory. Yep, this makes sense. The other programs running on managed platforms and. Or let's see, or through interpreters consume more memory. Although Python fares really well in this case, there is about an order of magnitude difference in memory consumption between these two groups. Yep. It is a surprise to me that dot net somehow has the worst footprint. But I guess that this can be tuned with some settings. Maybe let me know in the comments. Okay, this is good. It's good that he's stating where he's a little bit surprised about. I like to see that. All right, so 10k tasks before I. Oh no. Oh no. You can see. You can see what's happening here. All right, so let's see. Rust threads. This. This makes. This makes perfect sense, right? It is expensive. So I guess one thing that they. We didn't specify here or that he didn't speak about with these two right here, which was how much worker threads were created. Right. So I don't know what Tokyo does, but there is definitely something to that. Right? Thread stack size, right? Yeah, there's a. There's a lot here that might be hidden that we may not be considering correctly. There's just. It just looks really small, which it may not be. This again, this is one of the problems about making a really, really small synthetic test is that you don't know what's going to get you. This makes more sense. This is something I, I guess I, I can believe Go being this just because GO does have a whole managed system around it. And so this is good. This is still great. This is less than what it took a thousand or ten thousand go threads. GO routines. Go Go co routines is less memory than node JS by itself. Right. So it's pretty good. Pretty dang good. I like to see this. Let's see Java virtual threads. Yeah, this makes sense. I'm a little bit surprised that Java is that big for regular threads. Right. C sharp, I assume. Doesn't change much. Yeah, it doesn't change pretty much at all because again, it's probably doing the same Tokyo thing that's going on here. Added nothing. Right. So this makes sense. Virtual threads. Also, this is, you know, slightly in line. This is more in line with Go, I guess. Node. I am very curious how that one. I guess it's because set timeout is not really. So one thing that he did not do correctly is it's not. No doesn't really have this concept of threads. Right. I. I guess maybe the most correct way you could do this would to be. Do. Would to do something like worker workers. Right. And the reason why this is kind of odd and is that no just creates a. A a event loop item and that event loop item is probably some very small piece of information. My guess is that especially since the timer is probably a time for when it's done and a pointer to a function to call. Right. It's like going to be a pretty dang small amount of memory and so this makes sense, that node really doesn't do much because you actually didn't create multiple threads. You created 10,000 timers, which is much, much different because all of these through here, they can, they can also execute with parallelism. You know what I mean? There's parallelism that can go on in these that cannot happen in Node js, therefore, they're not really equal. So it'd have to be worker threads. You'd have to use something like worker threads. I don't know if Python has the same parallelism problem. I don't really know how Python works, and I also don't know how Elixir works, but good job, Elixir. Right? I said parallelism, parallelism intentionally because these can all execute with parallel parallelism. Parallel parallelism, depending on how many worker threads. Where's Gradle? Great question. All right, few surprises. Everybody probably expected the threads would be a big loser of this benchmark. And this is true for Java threads, which indeed consumed about 250 megabytes of RAM. The native Linux threads used from Russ seem to be lightweight enough that the 10k threads of memory consumption is still lower than the idle memory consumption of many other runtimes. Async tasks or virtual green threads might be lighter than native threads. I would say might is, I mean, observably probably true is what we're seeing here, right? I'd say it's observably true, but we don't see the advantage at only 10k tasks. We need more tasks. Okay. Another surprise here is Go. GO routines are supposed to be very lightweight, but they're actually consumed more than 50 of the RAM required by Rust threads. Honestly, I was expecting much bigger difference in favor of Go. Hence I concluded, concluded that 10k current tasks threads are quite competitive. Alternative Linux kernel definitely does something right here. Again, I don't, Again, I, I, I'm not sure how much I buy this. I don't know. I don't know what GO does. You know what I mean? I don't know what GO does that makes this good or bad. GO may be reserving more memory. It may have different parameters than something like Tokyo does. And so again, I don't know how fair this is to say that Rust greatly outperformed it because it's not doing anything. Too much telemetry. Too much telemetry. All right. GO also has lost its advantage over Rust Async in the previous benchmark and now consumes over six more times more memory than the best Rust program, which is overtaken By Python. Yeah. The final surprise is that 10k task memory consumption on. Net didn't significantly go up from the idle memory used. Yeah, again, telemetry. No telemetry. It's actually just telemetry. Probably it just uses reallocated memory or its idle memory is just so high that 10k did. Yeah, didn't matter. Okay. 100,000 tasks. Okay, let's do this. Let's see. So the thread benchmarks could be excluded. Probably this could have somehow tweaked by changing system settings. But after about an hour I gave up. So here's a hundred thousand tasks. Okay. So yeah, you can't spawn non. There you go. So this is a good. Notice that all non green threads have all gone away. So I guess. My guess is that his program kept crashing. He probably. I would assume your U limit. You should be able to do you. I don't know if there's like. I don't know what the potential requirements are that you can't spawn a hundred thousand threads, but my guess is that you just fork bomb yourself. It explodes and dies. Alright, so Tokyo's gone up. This has gone up. This has gone up. Like to me this is pretty. These are all pretty fine. Again, I really doubt C Sharp's doing something right. Something about C Sharp tells me that this is not executing the way you think it is. Can we all agree that this is not doing what you think it is? There's no way that you just did ten thousand. You. There's no way that you did one to ten thousand to a hundred thousand with absolutely no memory change. Something is being clever here, right? Something's being very clever. Or C Sharp is probably the best. No js. So C Sharp's gonna win. I hope everybody sees this coming. Right? I hope everybody sees this coming. That C sharp is going to start beating out some rust if they keep going with thread limits. Alright. At this point, Go program has been beaten up not only by Rust, but also by Java, C Sharp and no JS. Let's see. And Linux.net likely cheats because its memory use still isn't going up. I had to double check if it really launches the right number of tasks, but indeed it does and still exits after about 10 seconds. It doesn't block the main loop. Okay. I would still, I would argue you need to do something. I bet you this will greatly change in. In C Sharp. If you had like a. A concurrent hashmap that every one of those tasks try to add one item to and read one item from. I think it would Just completely change the memory wildly. Right, let's see. Okay, one million tasks. Let's go extreme, extreme, extreme, extreme. All right. At 1 million tasks, Alyssa gave up. Okay, nice. System limit has been reached. Okay, Edit. Some commenters pointed out that I could have increased the limit. Yep, you limit. After adding a roll P plus bajillion to Elixir, it ran fine. Okay, nice. All right, let's see what we got here. Nice. Look at. Look at that. C Sharp. Oh, memory did go up this time. So that's interesting. C Sharp's memory did go up. I Wonder why this 10x caused memory to go up, but the other two 10x's didn't. Little sus. Little sus. And C sharp's the best, everybody. Finally we see an increase in memory consumption of the C Sharp program. But still competitive. It even managed to slightly beat one of the Rust run times. The distance between Go and the others increased. Now go loses over 12x to the winner. It also loses 2x to Java Java, which contradicts a general perception of JVM being a memory hog and Go being lightweight. Rust Hokyo remained unbeatable. This isn't surprising after seeing how it did 100k tasks. Final word. As we observed, a high number of concurrent tasks. Actually, I want to have a final word first. Okay, I'm having the final word first. First off, I don't know if I like this benchmark. I love the idea. I don't know if I love the benchmark. I feel like you need to do more things, right? I really do feel like you need to do more things for this to be real, because something is wrong here. First off, one thing about C Sharp and memory that they're completely just disregarding along with Java is garbage collection along with Node, along with Go. Like, all of these have garbage collection. So does Python. I assume Elixir does too. But, I mean, Elixir is already at four gigs. Am I right? Am I right? But that's where Rust is going to really shine, is that if you're just measuring memory, doing something that creates and uses memory, these other ones are going to really struggle. But I wonder how much Go is going to struggle, because Go gets the best of two worlds. It gets a managed memory environment, but it also gets, like, the smallness of Rust when it comes to usage of memory. So when you create a struct, you're getting, like, a small, smaller structure. You're not getting a node JS struct, which is just much different. Right? An object in Node is Not going to be nearly as lightweight as an object in Go. It just. That's how it works. And so there is like. It's kind of interesting. You know what I mean? It's just. It's just interesting that doing nothing. This is the results. But I just don't believe it because my guess is that this elixir number four gigabytes, is likely what would happen to a lot of these if you did that. All right, final word. What is wrong? Why do I keep having like. As we have observed, a high number of concurrent tasks can consume a significant amount of memory, even if it did not do not perform complex operations. Yeah, I mean, it makes sense. Like just imagine that every single. Imagine that every single task ran requires 100 bytes of memory. That would be 100. That'd be 100 megabytes at a million. Right? So it likely is going to require more than that. Yeah, stack size, you got all sorts of stuff. It requires probably more than a hundred bytes. Therefore it makes sense. This makes sense, right? This is like what's a million times 4k, right? If you had 4k stack size, boom. You got that right. Anyways, let's see. Conversely, other run times with high initial overhead can handle high workloads effortlessly. C Sharp for the win, by the way. By the way, the big takeaway here is you should just use C Sharp. We're all C Sharp Andes now inside this stream. I hope everybody's ready for the C Sharp arc. C Sharp Arc. Everyone excited for it? I'm excited for it. I think that C Sharp's obviously the best language. Okay, I've been telling you guys this for so long now, and honestly, this chart just proves how good C Sharp is. Okay, you guys kept always talking about how great GO is. Look at how terrible go is. 2.66 gigabytes. Okay? You just don't understand things at all. All right. C Sharp. Clearly best language. Clearly best language. The comparison focused solely on memory consumption, while other factors such as task launch time and communication speeds are equally important. Notably, at 1 million tasks, I observed that the overhead of launching tasks became evident and most programs required more than 12 seconds to complete. Stay tuned for upcoming benchmarks where I explore additional aspects in depth. I'd love to see this. Except for instead of doing Node JS, just doing timers, let's set worker threads. I'd love to see some sort of computation model added to everything. I personally just think that the best way to do this is to do some sort of longer run living task. Right? How many websocket connections can you make? How many effectively how many open TCP connections can you make to a server and then just start sending something back and forth. How much can you do in a language not like these. Because you know the reality is you don't use a language to launch a timer. You use a language to do something. And I feel like a web sockets like a really great kind of. It's a really great simple way to test something because it is. It's just a TCP connection. You have to do a moderate a pretty small amount of work to parse out a frame. It shows what garbage collection does to the system and uh, it shows what kind of the interacting with with the system calls does to a system. And then if you have anything extra on the system such as like if you do a chat room it will you know if you have to for each over clients what do for loops do to your program. It's very very interesting uh even with no JS the difference like if you build a chat client and all it is is a chat client where websocket connections can join in. Make a simple request to join rooms and then send messages to the room A for each statement will effectively cut your RPS in half when you're iterating over the available sockets. Or it's not rps, it's NPS messages per second. Pretty wild that that that can happen, right? And so it's. It's pretty wild that even such a simple small little thing can have such a huge impact on performance. Anyways, just something to think about any. All right. Hey, great article though. Great article everybody. Hey everybody. Great article. Hey, great article everybody. Everybody give a little clap for Peter. Good job Peter. Appreciate the work you put in. What is the name? You know what the hell the name is? The name is the Primogen.\n",
      "Status: completed\n",
      "Transcription completed!\n",
      "\n",
      "==================================================\n",
      "VIDEO METADATA:\n",
      "==================================================\n",
      "Title: How Much Memory for 1,000,000 Threads in 7 Languages | Go, Rust, C#, Elixir, Java, Node, Python\n",
      "Description: Recorded live on twitch, GET IN \n",
      "\n",
      "https://twitch.tv/ThePrimeagen\n",
      "\n",
      "ty piotr!\n",
      "https://pkolaczk.github.io/memory-consumption-of-async/\n",
      "\n",
      "MY MAIN YT CHANNEL: Has well edited engineering videos\n",
      "https://youtube.com/ThePrimeagen\n",
      "\n",
      "Discord\n",
      "https://discord.gg/ThePrimeagen\n",
      "\n",
      "\n",
      "Have something for me to read or react to?: https://www.reddit.com/r/ThePrimeagenReact/\n",
      "Uploader: ThePrimeTime\n",
      "Tags: ['programming', 'computer', 'software', 'software engineer', 'software engineering', 'program', 'development', 'developing', 'developer', 'developers', 'web design', 'web developer', 'web development', 'programmer humor', 'humor', 'memes', 'software memes', 'engineer', 'engineering', 'Regex', 'regexs', 'regexes', 'netflix', 'vscode', 'vscode engineer', 'vscode plugins', 'Lenovo', 'customer service']\n",
      "\n",
      "==================================================\n",
      "TRANSCRIPT:\n",
      "==================================================\n",
      "How much memory do you need to run 1 million concurrent tasks? By the way, alerts are off. In this blog post, I delve into the comparison of memory consumption between asynchronous and multi threaded programs across popular languages like Rust, Go, Java, C Sharp, Python, no js, and of course, everybody's favorite. These nuts. Sometimes ago I had to compare performance of a few computer programs designed to handle a large number of network connections. I saw a huge difference in memory consumption of those programs even exceeding 20x. Okay, that. I mean, it makes sense. Well, you know, how are they handling things? Because like in Go, it's pretty. It's pretty slick. Goes pretty dang. Pretty dang slick. You know what I mean? Some programmers consume little over 100 megabytes, but others reach almost three gigs. I think what he's trying to say here, I think what Peter. What old Peter up there, Kalaskowski, is trying to say is that node js, node js loves the memory, okay? At 10k connections. Unfortunately, those programs were quite complex and differed also in features. So it'd be hard to compare them directly and draw some meaningful conclusion as that wouldn't be an Apple to Apple comparison. This led me to an idea of creating a synthetic benchmark instead. Synthetic. By the way, I actually do want to like follow this, this little line of thought right here. So I want everyone to pay really close attention. If you go to GitHub.com the Primogen, right? And you look at this and you go to my latest one. Is there like. Yeah, not left pad. There we go. T.S. rust. Zig. D's. Yes. I'm. I'm going through a D's phase. Okay. If you go here, you can join and add your own language. We're building the interpreter, okay. And then I want to test it. I actually want to build a server and that does like full interpretation and we'll figure out what the, what the rules are and all that, but actually can compile remotely, right? And then that is what language can do that the fastest. I feel like that is a way cooler, like, test of languages than these dumb tests that you see, right? There's that one YouTube video that did like, that just did like a mathematical formula and be like, which one does this one the best? And it's just like. But that's not even like real at all. Okay, you're not even creating mem. We need like, like memory. You need cleanup, you need things that happen. You need connections, you need syscalls. You want to see the entire, you know, the entire Thing you don't want to just see. Like just some tiny little nothing. You know what I mean? It never works that way. It's always a lie. It's just a lie. It's just always a lie. Anyways, let's do this. Yeah. Microsoft guy. Yeah. Benchmark. I created the following program in various programming languages. Let's launch n concurrent tasks where each tackle waits for 10 seconds and then the program exists after all the tasks finish. What? The program continues to exist after you're finished. I'm pretty sure this is like a fork bomb. Then the number of tasks is controlled by the command line argument. Let's go. Let's go. The Little help of Chat Jeopardy. I could write such programs in a few. What? What? What do you mean this is not hard. And Rust. I mean are you using a real threat? Are using hardware threads or green threads or Go? That's simple. Java. This is probably not that hard. Could probably do it in a few moments of just a. What do you mean chat? Shippity. See this seems like such a crazy crazy things. People just love them. Chad Jippities Anyways, Rust. I created three programs in Rust. The first one uses traditional threads. That's what I wanted to see. Okay. I wanted to see one hardware in one language. Traditional threads. Okay. I'm six of all this polythreadisms going on. Here's the core of it. Bam. Make it happen. The other two versions had Async with Tokyo and the other with Async stood. Here's the core of the Tokyo. Beautiful. Beautiful. I like this. The Async stud variant is very similar, so I won't put it there. Go Go routines are the building block for conservancy so we don't need to do them separately. But we used a weight group. Okay. Beautiful. Great use of a weight group by the way. This is a great use of a weight Awake group. Loved it. Loved every part of this. Again, I think Go for all of its downfalls also has a lot of updrafts. What's. What's the term for like what's the opposite of a downfall? Yes, I did have a bad experience with C Sharp. Why you bring that up? Okay. An upfall, an upwind, an updraft, an uplift, a boob job. What do we call it? I don't even know Java. Java traditionally uses threads, but JD JDK21 offers a preview of virtual threads which are similar concept to Go routines. Look at Java Go. Java has almost caught up to 2015. I think we all need to Be impressed right now. Can we just take a moment to realize that Java's going places? It really is like this is incredible. I mean the next thing you know they, I mean the next. They could actually beat C. By the time C23 comes out, Java might actually be exceeding what happened in 2023. As far as techno technologies go, it could be, it could be incredible. And I think you guys are just not even considering how amazing this is. All right, let's see. List of threads, new array list. Good choice of arraylist. You did not create, you know, a little alarming that you didn't, you know, pre populate the size here. You knew how big it was going to be. A bunch of things. New threads. Alright. Little sleep, little dry catch does it. So he's not counting. One thing I'd be a little careful of is even though you don't need to, there is no counting right here going on. Right. How many times did this fail? I think that's really important to think about. You know, just in case when you're creating these tests you, you need to do that. Okay, thread start, thread add. Let's go. Thread join. Perfect. Beautiful. I'm curious about this as well. When you go individual thread joins, this could be bad, right? Am I, am I right on this one? This could actually have a, a greater slowdown because you're, you're doing it one at a time and so you could have already had like a hundred finished that you like aren't. You know what I mean? That's a race. Yeah, yeah. I, I feel like this is, it's blocking. I know that, but it's, it's not just blocking, it's that you could pay a huge penalty up front. Then you'll have this whole thing where you have to go through each one and I assume also join is like not a free call. I assume it's like a syscall. Is that fair? I don't know. I don't know what it takes to do a join. But my, my assumption is that it's a non trivial cost, you know? I mean. Yeah. Anyways, just a thought. Just a thought. But I guess if they're green threads, maybe that's not the case. Maybe they're actually really fast and delicious. You know what I mean? I don't know. Anyways, these are just my thoughts. I'm just kind of raw, dogging my thoughts out here. Okay, here's the, the variant with virtual threads. Notice how similar they are. Oh, nice. That's beautiful. Well done. Oh, I guess these Ones are hardware ones and the other one. These ones are hardware ones. These ones are. Oh, my goodness, I'm scrolling way too fast. Oh, my goodness, I'm gonna vomit. These ones are virtual ones. Beautiful. Still doing that. I don't like that. C Sharp is similar to Rust, has first class support for Async Await. Awesome. So there we go. We're gonna do a little task run right here. So this must be. Okay, so this has to be. Those have to be synthetic threads. So we're not actually getting. So since C Sharp is not doing hardware threads, so there does need to be like a. A disambiguation here. Which one's doing, like actual. Actual threads versus which ones are doing some sort of managed environment. Because I would assume that the managed environment. This is where a managed environment does. Amazing. Right? Right? I think so. It can do both. Well, I would assume whenever you see something like Async Await, usually there's some sort of whole managed environment that goes into it. Usually. Right. Because if you're not doing it yourself, there's something else going on. You know what I mean? I feel like the managed environment would be slower. No, because hardware. Hardware threads are expensive. Right? No. J.S. utils promised. Okay, so again, you don't want to do this. I feel like there is a little something there you got to be careful about. You know, maybe a little extra garbage collection going on. I don't know. But probably fine. Probably not a big deal. Probably enough. Probably not. Oh, my goodness. Probably not enough to be too upset about it. Python. I don't know what delay is. Delay must just be a function that returns a. Oh, set timeout. Oh, yeah. Okay. Yeah, yeah. I'm dumb. I'm dumb. This is also a syntax error, guys. Syntax error right there. Okay. Watch your parents. Okay. Don't. You should make sure you always just put code up on an article. That's just like. It works. Always make sure. I mean, I've done it too. We've all done it. Just make sure it works. Right? Okay, so this is looking good. Python added Async Await. And three, five. Nice. Okay, look at this. Asyncio. Asynchronous. There we go. Beautiful. That all looks great. Elixir is famous for Async capabilities as well. Let's go. I love to see it. Task, await, many, until infinity and beyond. You know, I really hate that phrase by Buzz Lightyear. To infinity and beyond. Wouldn't you not be at infinity if you could go beyond the point of infinity? Isn't that just like. Not infinity like can we be real here? That's the joke. It's true also Infinity. Yeah, I know. Thank you. Deep thoughts with me. Just letting you know. Deep thoughts. All right. Test environment. Hardware. Xeon. This thing. Okay, this is starting to look like a personal computer, which again, gotta be a little careful. Rust 169. Nice go. 18. 1. Nice. JDK dotnet node, Python Elixir. Let's go. All programs were launched using release mode if available. Other options were left default. All right, minimum footprint. Let's start with something small because some of the runtimes require some memory for themselves. Yep, that's right. Yep. Here we go. This makes sense though, because Node would be really, really large. Well, C Sharp requires 131 megabytes for just nothing. What Like I get that there's like a whole thing going on, but that's just a lot. I mean this all seems about in line with Node, right? C Sharp. I want. What does C Sharp do that requires three times the amount of telemetry information that the other ones do? Okay, it's Pindos anyways, so go. Make sense, Rust. All these things make sense, right? Because they all should be really, really small because they're actually. You know, these are actually like compiled thingies. But I would like to say that this is really impressive for Go. I want you to take a. A moment about this. This includes a garbage collector. Okay. Like things are running here. That's really good. That's really, really good. Good job Go. Good job, Go. That's really, really good. The. The surprise here was Python. It shouldn't be surprising. Oh, I mean, I guess it's surprising in the sense that it's half the. It's half the memory of. I guess I would have. I guess in my head I would have probably put it the same as Java or no JS Elixir. Also a little surprising. It's so large. Interesting. I wonder how he's getting the memory. Is he using vm rss? What is he using? Here we can see there's are certainly two groups of programs. Go and Rust. Programs compile statically to native binaries need very little memory. Yep, this makes sense. The other programs running on managed platforms and. Or let's see, or through interpreters consume more memory. Although Python fares really well in this case, there is about an order of magnitude difference in memory consumption between these two groups. Yep. It is a surprise to me that dot net somehow has the worst footprint. But I guess that this can be tuned with some settings. Maybe let me know in the comments. Okay, this is good. It's good that he's stating where he's a little bit surprised about. I like to see that. All right, so 10k tasks before I. Oh no. Oh no. You can see. You can see what's happening here. All right, so let's see. Rust threads. This. This makes. This makes perfect sense, right? It is expensive. So I guess one thing that they. We didn't specify here or that he didn't speak about with these two right here, which was how much worker threads were created. Right. So I don't know what Tokyo does, but there is definitely something to that. Right? Thread stack size, right? Yeah, there's a. There's a lot here that might be hidden that we may not be considering correctly. There's just. It just looks really small, which it may not be. This again, this is one of the problems about making a really, really small synthetic test is that you don't know what's going to get you. This makes more sense. This is something I, I guess I, I can believe Go being this just because GO does have a whole managed system around it. And so this is good. This is still great. This is less than what it took a thousand or ten thousand go threads. GO routines. Go Go co routines is less memory than node JS by itself. Right. So it's pretty good. Pretty dang good. I like to see this. Let's see Java virtual threads. Yeah, this makes sense. I'm a little bit surprised that Java is that big for regular threads. Right. C sharp, I assume. Doesn't change much. Yeah, it doesn't change pretty much at all because again, it's probably doing the same Tokyo thing that's going on here. Added nothing. Right. So this makes sense. Virtual threads. Also, this is, you know, slightly in line. This is more in line with Go, I guess. Node. I am very curious how that one. I guess it's because set timeout is not really. So one thing that he did not do correctly is it's not. No doesn't really have this concept of threads. Right. I. I guess maybe the most correct way you could do this would to be. Do. Would to do something like worker workers. Right. And the reason why this is kind of odd and is that no just creates a. A a event loop item and that event loop item is probably some very small piece of information. My guess is that especially since the timer is probably a time for when it's done and a pointer to a function to call. Right. It's like going to be a pretty dang small amount of memory and so this makes sense, that node really doesn't do much because you actually didn't create multiple threads. You created 10,000 timers, which is much, much different because all of these through here, they can, they can also execute with parallelism. You know what I mean? There's parallelism that can go on in these that cannot happen in Node js, therefore, they're not really equal. So it'd have to be worker threads. You'd have to use something like worker threads. I don't know if Python has the same parallelism problem. I don't really know how Python works, and I also don't know how Elixir works, but good job, Elixir. Right? I said parallelism, parallelism intentionally because these can all execute with parallel parallelism. Parallel parallelism, depending on how many worker threads. Where's Gradle? Great question. All right, few surprises. Everybody probably expected the threads would be a big loser of this benchmark. And this is true for Java threads, which indeed consumed about 250 megabytes of RAM. The native Linux threads used from Russ seem to be lightweight enough that the 10k threads of memory consumption is still lower than the idle memory consumption of many other runtimes. Async tasks or virtual green threads might be lighter than native threads. I would say might is, I mean, observably probably true is what we're seeing here, right? I'd say it's observably true, but we don't see the advantage at only 10k tasks. We need more tasks. Okay. Another surprise here is Go. GO routines are supposed to be very lightweight, but they're actually consumed more than 50 of the RAM required by Rust threads. Honestly, I was expecting much bigger difference in favor of Go. Hence I concluded, concluded that 10k current tasks threads are quite competitive. Alternative Linux kernel definitely does something right here. Again, I don't, Again, I, I, I'm not sure how much I buy this. I don't know. I don't know what GO does. You know what I mean? I don't know what GO does that makes this good or bad. GO may be reserving more memory. It may have different parameters than something like Tokyo does. And so again, I don't know how fair this is to say that Rust greatly outperformed it because it's not doing anything. Too much telemetry. Too much telemetry. All right. GO also has lost its advantage over Rust Async in the previous benchmark and now consumes over six more times more memory than the best Rust program, which is overtaken By Python. Yeah. The final surprise is that 10k task memory consumption on. Net didn't significantly go up from the idle memory used. Yeah, again, telemetry. No telemetry. It's actually just telemetry. Probably it just uses reallocated memory or its idle memory is just so high that 10k did. Yeah, didn't matter. Okay. 100,000 tasks. Okay, let's do this. Let's see. So the thread benchmarks could be excluded. Probably this could have somehow tweaked by changing system settings. But after about an hour I gave up. So here's a hundred thousand tasks. Okay. So yeah, you can't spawn non. There you go. So this is a good. Notice that all non green threads have all gone away. So I guess. My guess is that his program kept crashing. He probably. I would assume your U limit. You should be able to do you. I don't know if there's like. I don't know what the potential requirements are that you can't spawn a hundred thousand threads, but my guess is that you just fork bomb yourself. It explodes and dies. Alright, so Tokyo's gone up. This has gone up. This has gone up. Like to me this is pretty. These are all pretty fine. Again, I really doubt C Sharp's doing something right. Something about C Sharp tells me that this is not executing the way you think it is. Can we all agree that this is not doing what you think it is? There's no way that you just did ten thousand. You. There's no way that you did one to ten thousand to a hundred thousand with absolutely no memory change. Something is being clever here, right? Something's being very clever. Or C Sharp is probably the best. No js. So C Sharp's gonna win. I hope everybody sees this coming. Right? I hope everybody sees this coming. That C sharp is going to start beating out some rust if they keep going with thread limits. Alright. At this point, Go program has been beaten up not only by Rust, but also by Java, C Sharp and no JS. Let's see. And Linux.net likely cheats because its memory use still isn't going up. I had to double check if it really launches the right number of tasks, but indeed it does and still exits after about 10 seconds. It doesn't block the main loop. Okay. I would still, I would argue you need to do something. I bet you this will greatly change in. In C Sharp. If you had like a. A concurrent hashmap that every one of those tasks try to add one item to and read one item from. I think it would Just completely change the memory wildly. Right, let's see. Okay, one million tasks. Let's go extreme, extreme, extreme, extreme. All right. At 1 million tasks, Alyssa gave up. Okay, nice. System limit has been reached. Okay, Edit. Some commenters pointed out that I could have increased the limit. Yep, you limit. After adding a roll P plus bajillion to Elixir, it ran fine. Okay, nice. All right, let's see what we got here. Nice. Look at. Look at that. C Sharp. Oh, memory did go up this time. So that's interesting. C Sharp's memory did go up. I Wonder why this 10x caused memory to go up, but the other two 10x's didn't. Little sus. Little sus. And C sharp's the best, everybody. Finally we see an increase in memory consumption of the C Sharp program. But still competitive. It even managed to slightly beat one of the Rust run times. The distance between Go and the others increased. Now go loses over 12x to the winner. It also loses 2x to Java Java, which contradicts a general perception of JVM being a memory hog and Go being lightweight. Rust Hokyo remained unbeatable. This isn't surprising after seeing how it did 100k tasks. Final word. As we observed, a high number of concurrent tasks. Actually, I want to have a final word first. Okay, I'm having the final word first. First off, I don't know if I like this benchmark. I love the idea. I don't know if I love the benchmark. I feel like you need to do more things, right? I really do feel like you need to do more things for this to be real, because something is wrong here. First off, one thing about C Sharp and memory that they're completely just disregarding along with Java is garbage collection along with Node, along with Go. Like, all of these have garbage collection. So does Python. I assume Elixir does too. But, I mean, Elixir is already at four gigs. Am I right? Am I right? But that's where Rust is going to really shine, is that if you're just measuring memory, doing something that creates and uses memory, these other ones are going to really struggle. But I wonder how much Go is going to struggle, because Go gets the best of two worlds. It gets a managed memory environment, but it also gets, like, the smallness of Rust when it comes to usage of memory. So when you create a struct, you're getting, like, a small, smaller structure. You're not getting a node JS struct, which is just much different. Right? An object in Node is Not going to be nearly as lightweight as an object in Go. It just. That's how it works. And so there is like. It's kind of interesting. You know what I mean? It's just. It's just interesting that doing nothing. This is the results. But I just don't believe it because my guess is that this elixir number four gigabytes, is likely what would happen to a lot of these if you did that. All right, final word. What is wrong? Why do I keep having like. As we have observed, a high number of concurrent tasks can consume a significant amount of memory, even if it did not do not perform complex operations. Yeah, I mean, it makes sense. Like just imagine that every single. Imagine that every single task ran requires 100 bytes of memory. That would be 100. That'd be 100 megabytes at a million. Right? So it likely is going to require more than that. Yeah, stack size, you got all sorts of stuff. It requires probably more than a hundred bytes. Therefore it makes sense. This makes sense, right? This is like what's a million times 4k, right? If you had 4k stack size, boom. You got that right. Anyways, let's see. Conversely, other run times with high initial overhead can handle high workloads effortlessly. C Sharp for the win, by the way. By the way, the big takeaway here is you should just use C Sharp. We're all C Sharp Andes now inside this stream. I hope everybody's ready for the C Sharp arc. C Sharp Arc. Everyone excited for it? I'm excited for it. I think that C Sharp's obviously the best language. Okay, I've been telling you guys this for so long now, and honestly, this chart just proves how good C Sharp is. Okay, you guys kept always talking about how great GO is. Look at how terrible go is. 2.66 gigabytes. Okay? You just don't understand things at all. All right. C Sharp. Clearly best language. Clearly best language. The comparison focused solely on memory consumption, while other factors such as task launch time and communication speeds are equally important. Notably, at 1 million tasks, I observed that the overhead of launching tasks became evident and most programs required more than 12 seconds to complete. Stay tuned for upcoming benchmarks where I explore additional aspects in depth. I'd love to see this. Except for instead of doing Node JS, just doing timers, let's set worker threads. I'd love to see some sort of computation model added to everything. I personally just think that the best way to do this is to do some sort of longer run living task. Right? How many websocket connections can you make? How many effectively how many open TCP connections can you make to a server and then just start sending something back and forth. How much can you do in a language not like these. Because you know the reality is you don't use a language to launch a timer. You use a language to do something. And I feel like a web sockets like a really great kind of. It's a really great simple way to test something because it is. It's just a TCP connection. You have to do a moderate a pretty small amount of work to parse out a frame. It shows what garbage collection does to the system and uh, it shows what kind of the interacting with with the system calls does to a system. And then if you have anything extra on the system such as like if you do a chat room it will you know if you have to for each over clients what do for loops do to your program. It's very very interesting uh even with no JS the difference like if you build a chat client and all it is is a chat client where websocket connections can join in. Make a simple request to join rooms and then send messages to the room A for each statement will effectively cut your RPS in half when you're iterating over the available sockets. Or it's not rps, it's NPS messages per second. Pretty wild that that that can happen, right? And so it's. It's pretty wild that even such a simple small little thing can have such a huge impact on performance. Anyways, just something to think about any. All right. Hey, great article though. Great article everybody. Hey everybody. Great article. Hey, great article everybody. Everybody give a little clap for Peter. Good job Peter. Appreciate the work you put in. What is the name? You know what the hell the name is? The name is the Primogen.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe a YouTube video\n",
    "print(\"Starting YouTube video transcription...\")\n",
    "transcript, video_info = transcribe_youtube_video(youtube_url)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VIDEO METADATA:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in video_info.items():\n",
    "    print(f\"{key.title()}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRANSCRIPT:\")\n",
    "print(\"=\"*50)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1cb5b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (1.90.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (8.1.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: certifi in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: decorator in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/chandreshkumar/code/reweave/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74973a6c676d415691aa0876f713dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"Title: How Much Memory for 1,000,000 Threads in 7 Languages | Go, Rust, C#, Elixir, Java, Node…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913e3b17782849bba7c3c21a106d6b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Screenplay', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020a402dc1114186b9778b49b8ef002c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install openai ipywidgets\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Prepare default context from metadata and transcript\n",
    "default_context = (\n",
    "    f\"Title: {video_info.get('title')}\\n\"\n",
    "    f\"Description: {video_info.get('description')}\\n\"\n",
    "    f\"Uploader: {video_info.get('uploader')}\\n\"\n",
    "    f\"Tags: {video_info.get('tags')}\\n\\n\"\n",
    "    \"Transcript:\\n\" + transcript\n",
    ")\n",
    "\n",
    "# Create editable text area for context\n",
    "context_input = widgets.Textarea(\n",
    "    value=default_context,\n",
    "    description='Context:',\n",
    "    layout=widgets.Layout(width='100%', height='300px')\n",
    ")\n",
    "display(context_input)\n",
    "\n",
    "# Button to generate screenplay\n",
    "generate_btn = widgets.Button(description='Generate Screenplay')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_generate_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # Construct prompt for LLM\n",
    "        prompt = (\n",
    "            \"Format the following context and transcript into a screenplay format, with scene headings, speaker names, and dialogue.\\n\\n\" \n",
    "            + context_input.value\n",
    "        )\n",
    "        # Call the new OpenAI client API\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that formats transcripts into screenplays.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        screenplay = response.choices[0].message.content\n",
    "        print(screenplay)\n",
    "\n",
    "# Display button and output\n",
    "generate_btn.on_click(on_generate_clicked)\n",
    "display(generate_btn, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "845458a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating concise bullet-point summary of the transcript...\n",
      "\n",
      "Summary:\n",
      "\n",
      "- The podcast revolves around comparing memory consumption between asynchronous and multi-threaded programs across popular languages like Rust, Go, Java, C Sharp, Python, and Node.js.\n",
      "- The host mentions a significant difference in memory consumption among different programs, with some consuming just over 100 megabytes and others reaching almost three gigabytes.\n",
      "- The host criticizes certain benchmarks for not being representative of real-world scenarios, suggesting that tests should involve more complex operations like memory cleanup, connections, and syscalls.\n",
      "- The host presents a program that launches n concurrent tasks, each waiting for 10 seconds, and then the program continues to exist after all tasks finish; the number of tasks is controlled by a command line argument.\n",
      "- The host discusses the performance of different languages in the program, noting that Rust and Go perform well, while C Sharp's memory consumption is surprisingly high.\n",
      "- The host suggests that the results may not be entirely accurate due to differences in how each language handles threads and memory management.\n",
      "- The host emphasizes that the memory consumption of a program can significantly increase with a high number of concurrent tasks, even if those tasks do not perform complex operations.\n",
      "- The host concludes by stating that other factors like task launch time and communication speed are equally important as memory consumption in assessing a language's performance.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the transcript into concise bullet points\n",
    "print(\"Generating concise bullet-point summary of the transcript...\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text into concise bullet points without omitting important details.\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            \"Here is the transcript from the podcast. \"\n",
    "            \"Please provide a concise summary in bullet-point format, covering all key points.\" +\n",
    "            f\"\\n\\n{transcript}\"\n",
    "        )}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "summary = response.choices[0].message.content\n",
    "print(\"\\nSummary:\\n\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
